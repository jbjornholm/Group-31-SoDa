{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests,os,time\n",
    "def ratelimit():\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(0.5) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='requests',session=False,path2selenium='',n_tries = 5,timeout=30):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Firefox(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, Name used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit()\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          self.log.flush()\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "          self.log.flush()\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit()\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "      self.log.flush()\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "    ## connector.browser.page_source will give you the html.\n",
    "      return None,call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = Connector('Super Log')\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "import nltk, nltk.sentiment, sklearn\n",
    "%matplotlib inline\n",
    "name=nltk.corpus.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRABE FEMALE FIRST NAMES AND OTHERS\n",
    "first_female=[]\n",
    "sur_female=[]\n",
    "full_f=[]\n",
    "for i in range(1,8):\n",
    "    url='https://imdb.com/list/ls022928836/?sort=list_order,asc&mode=detail&page={}'.format(i)\n",
    "    call='Exam, KU, Female Page {}'.format(i)\n",
    "    response,call_id = connector.get(url, call)\n",
    "    html = response.text\n",
    "    soup=str(html)\n",
    "    if i < 7:\n",
    "        for i in range(1,101):\n",
    "            first_female.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[1]))\n",
    "            sur_female.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[2:]))\n",
    "            full_f.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].lstrip()))\n",
    "    if i == 7:\n",
    "        for i in range(1,7):\n",
    "            first_female.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[1]))\n",
    "            sur_female.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[2:]))\n",
    "            full_f.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].lstrip()))\n",
    "# SCRABE MALE FIRST NAMES AND OTHERS\n",
    "first_male=[]\n",
    "sur_male=[]\n",
    "full_m=[]\n",
    "for i in range (1,8):\n",
    "    url='https://www.imdb.com/list/ls022928819/?sort=list_order,asc&mode=detail&page={}'.format(i)\n",
    "    call='Exam, KU, Male Page {}'.format(i)\n",
    "    response,call_id = connector.get(url, call)\n",
    "    html = response.text\n",
    "    soup=str(html)\n",
    "    if i < 7:\n",
    "        for i in range(1,101):\n",
    "            first_male.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[1]))\n",
    "            sur_male.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[2:]))\n",
    "            full_m.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].lstrip()))\n",
    "    if i == 7:\n",
    "        for i in range(1,82):\n",
    "            first_male.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[1]))\n",
    "            sur_male.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].split(' ')[2:]))\n",
    "            full_m.append((soup.split('div class=\"lister-list\"')[1].split('h3 class')[i].split('</span>\\n<a href=')[1].split('>')[1].split('\\n')[0].lstrip()))\n",
    "\n",
    "first_m=list(dict.fromkeys(first_male))\n",
    "first_f=list(dict.fromkeys(first_female))\n",
    "\n",
    "# REMOVING DUPLICATES BY MAKING INTO DICT AND BACK AGAIN\n",
    "\n",
    "male_name=name.words('male.txt')+first_m\n",
    "first_m_done=list(dict.fromkeys(male_name))\n",
    "\n",
    "female_name=name.words('female.txt')+first_f\n",
    "first_f_done=list(dict.fromkeys(female_name))\n",
    "\n",
    "# SCRABE FEMALE DIRECTOR NAMES\n",
    "female_name_director = [] \n",
    "url = 'https://www.imdb.com/list/ls003532091/?fbclid=IwAR36QhJh8EDYKPCh_9RTsJdkvjNf-c-xQxtGy8bUZz1E2HefYpzxWeFA5w0'\n",
    "call = 'Exam, KU{}'.format(i)\n",
    "response,call_id = connector.get(url, call)\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "soup= BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "for j in range(0,100): \n",
    "    tree_node = soup.findAll('h3')[j]\n",
    "    name = tree_node.text.split('\\n')[2].strip()\n",
    "    female_name_director.append(name)\n",
    "\n",
    "# SCRABE MALE DIRECTOR NAMES\n",
    "male_name_director = []\n",
    "url = 'https://www.imdb.com/list/ls058727091/'\n",
    "call = 'Exam, KU{}'.format(i)\n",
    "response,call_id = connector.get(url, call)\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "soup= BeautifulSoup(html,'html.parser')    \n",
    "for j in range(0,100): \n",
    "    tree_node = soup.findAll('h3')[j]\n",
    "    name = tree_node.text.split('\\n')[2].strip()\n",
    "    male_name_director.append(name)\n",
    "\n",
    "# Gendering neutral names and removing\n",
    "names='C:/Users/frede/OneDrive/Documents/GitHub/Group/Group-31-SoDa/Data/sorted_names.csv'\n",
    "sorted_name=pd.read_csv(names, sep=';')\n",
    "fem_sort=list(sorted_name['F'].str.lstrip())\n",
    "mal_sort=list(sorted_name['M'].str.lstrip())\n",
    "neu_sort=list(sorted_name['N'].str.lstrip())\n",
    "fem_remove = mal_sort + neu_sort\n",
    "mal_remove = fem_sort + neu_sort\n",
    "sort_f=[x for x in first_f_done if x not in fem_remove]\n",
    "sort_m=[x for x in first_m_done if x not in mal_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Create Lists\n",
    "gross=[]\n",
    "rating=[]\n",
    "movie_stars=[]\n",
    "director=[]\n",
    "year=[]\n",
    "name=[]\n",
    "genre=[]\n",
    "summary=[]\n",
    "\n",
    "for v in range (1,201):\n",
    "    url = 'https://www.imdb.com/search/keyword/?ref_=kw_ref_yr&mode=detail&page={}&title_type=movie&fbclid=IwAR3B7G9VdhKWjVvbFIPhdH9vGLZwmO_zzTeNlCj4whUMbn_RtS3g1g9FiUQ&release_date=1980%2C2019&sort=num_votes,desc'.format(v)\n",
    "    response,callid = connector.get(url,'Exam')\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    print(v)\n",
    "    for j in range(50):\n",
    "        box=soup.findAll('div',{\"class\":'lister-item mode-detail'})[j]\n",
    "# BOX OFFICE\n",
    "        if len(box.findAll(\"span\", {\"name\": \"nv\"})) < 2:\n",
    "            gross.append(None)\n",
    "        if len(box.findAll(\"span\", {\"name\": \"nv\"})) == 2:\n",
    "            gross.append(box.findAll(\"span\", {\"name\": \"nv\"})[1].text)\n",
    "# Rating\n",
    "        if box.findAll(\"div\", {\"class\": \"inline-block ratings-imdb-rating\"})==[]:\n",
    "            rating.append(None)\n",
    "        else:\n",
    "            primo=box.findAll(\"div\", {\"class\": \"inline-block ratings-imdb-rating\"})[0].text\n",
    "            rating.append(primo.split('\\n')[2])\n",
    "# Movie Stars\n",
    "        if box.findAll(\"p\", {\"class\":\"text-muted text-small\"})== []:\n",
    "            movie_stars.append('NaN')\n",
    "        else: \n",
    "            stars = box.findAll(\"p\", {\"class\":\"text-muted text-small\"})[1].text.strip()\n",
    "            if len(stars.split(':'))<3:\n",
    "                movie_stars.append('NaN')\n",
    "            else:\n",
    "                stars_1 = stars.split(':')[2].split('\\n')[1:]\n",
    "                movie_stars.append(stars_1)\n",
    "# Directors\n",
    "        di=box.findAll(\"p\", {\"class\": \"text-muted text-small\"})[1].text\n",
    "        if len(di.split('Stars')[0].split('\\n')) > 5:\n",
    "            director.append(di.split('Stars')[0].split('\\n')[2:4])#+primo.split('Stars')[0].split('\\n')[3])\n",
    "        else:\n",
    "            director.append(di.split('Stars')[0].split('\\n')[2])\n",
    "# Years\n",
    "        headline_j = soup.findAll('h3')[j] # search for the first headline: h1 tag. \n",
    "        #name = headline_i['class'][0].strip() # use the class attribute name as column name.\n",
    "        value = headline_j.text.strip() # extract text using build in method        \n",
    "        film = value.split('\\n')[2]\n",
    "        if len(film)> 6:\n",
    "            film = value.split(' ')[-1]\n",
    "        year.append(film[1:5])\n",
    "# Name\n",
    "        headline_j = soup.findAll('h3')[j] # search for the first headline: h1 tag. \n",
    "        #name = headline_i['class'][0].strip() # use the class attribute name as column name.\n",
    "        value = headline_j.text.strip() # extract text using build in method.\n",
    "        film = value.split('\\n')[1]\n",
    "        name.append(film)\n",
    "# Genre\n",
    "        if box.findAll(\"span\", {\"class\":\"genre\"}) ==[]:\n",
    "            genre.append(None)\n",
    "        else: \n",
    "            movie_genre = box.findAll(\"span\", {\"class\":\"genre\"})[0].text.strip()\n",
    "            genre.append(movie_genre)\n",
    "# Summary\n",
    "        if box.findAll(\"p\", {\"class\":\"\"})== []:\n",
    "            summary.append(None)\n",
    "        else: \n",
    "            movie_summary = box.findAll(\"p\", {\"class\":\"\"})[0].text.strip()\n",
    "            summary.append(movie_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(gross))\n",
    "print(len(rating))\n",
    "print(len(movie_stars))\n",
    "print(len(director))\n",
    "print(len(year))\n",
    "print(len(name))\n",
    "print(len(genre))\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name=pd.DataFrame(name)\n",
    "df_year=pd.DataFrame(year)\n",
    "df_genre=pd.DataFrame(genre)\n",
    "df_rating=pd.DataFrame(rating)\n",
    "df_gross=pd.DataFrame(gross)\n",
    "df_director=pd.DataFrame(director)\n",
    "df_stars=pd.DataFrame(movie_stars)\n",
    "df_summary=pd.DataFrame(summary)\n",
    "df_name['year']=df_year\n",
    "df_name['genre']=df_genre\n",
    "df_name['rating']=df_rating\n",
    "df_name['gross']=df_gross\n",
    "df_name['director']=df_director\n",
    "df_name['summary']=df_summary\n",
    "df_name['Lead']=df_stars[0]\n",
    "df_name['Star 2']=df_stars[1]\n",
    "df_name['Star 3']=df_stars[2]\n",
    "df_name['Star 4']=df_stars[3]\n",
    "done=df_name\n",
    "done.columns=['Title','Year','Genre','Rating','Gross','Director','Summary','Lead','Star 2','Star 3','Star 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Director</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Lead</th>\n",
       "      <th>Star 2</th>\n",
       "      <th>Star 3</th>\n",
       "      <th>Star 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>Sex Drive</td>\n",
       "      <td>2008</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>6.5</td>\n",
       "      <td>$8.40M</td>\n",
       "      <td>Sean Anders</td>\n",
       "      <td>A high school senior drives cross-country with...</td>\n",
       "      <td>Josh Zuckerman,</td>\n",
       "      <td>Clark Duke,</td>\n",
       "      <td>Amanda Crew,</td>\n",
       "      <td>James Marsden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1981</td>\n",
       "      <td>Goon</td>\n",
       "      <td>2011</td>\n",
       "      <td>Comedy, Drama, Sport</td>\n",
       "      <td>6.8</td>\n",
       "      <td>$4.17M</td>\n",
       "      <td>Michael Dowse</td>\n",
       "      <td>Labeled an outcast by his brainy family, a bou...</td>\n",
       "      <td>Seann William Scott,</td>\n",
       "      <td>Jay Baruchel,</td>\n",
       "      <td>Alison Pill,</td>\n",
       "      <td>Eugene Levy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1982</td>\n",
       "      <td>Skønheden i alting</td>\n",
       "      <td>2016</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>6.8</td>\n",
       "      <td>$30.98M</td>\n",
       "      <td>David Frankel</td>\n",
       "      <td>Retreating from life after a tragedy, a man qu...</td>\n",
       "      <td>Will Smith,</td>\n",
       "      <td>Edward Norton,</td>\n",
       "      <td>Kate Winslet,</td>\n",
       "      <td>Michael Peña</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1983</td>\n",
       "      <td>Cop Land</td>\n",
       "      <td>1997</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>6.9</td>\n",
       "      <td>$44.89M</td>\n",
       "      <td>James Mangold</td>\n",
       "      <td>The Sheriff of a suburban New Jersey community...</td>\n",
       "      <td>Sylvester Stallone,</td>\n",
       "      <td>Harvey Keitel,</td>\n",
       "      <td>Ray Liotta,</td>\n",
       "      <td>Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>Creed II</td>\n",
       "      <td>2018</td>\n",
       "      <td>Drama, Sport</td>\n",
       "      <td>7.2</td>\n",
       "      <td>$115.72M</td>\n",
       "      <td>Steven Caple Jr.</td>\n",
       "      <td>Under the tutelage of Rocky Balboa, newly crow...</td>\n",
       "      <td>Michael B. Jordan,</td>\n",
       "      <td>Sylvester Stallone,</td>\n",
       "      <td>Tessa Thompson,</td>\n",
       "      <td>Phylicia Rashad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>Mississippi burning</td>\n",
       "      <td>1988</td>\n",
       "      <td>Crime, Drama, History</td>\n",
       "      <td>7.8</td>\n",
       "      <td>$34.60M</td>\n",
       "      <td>Alan Parker</td>\n",
       "      <td>Two F.B.I. Agents, with wildly different style...</td>\n",
       "      <td>Gene Hackman,</td>\n",
       "      <td>Willem Dafoe,</td>\n",
       "      <td>Frances McDormand,</td>\n",
       "      <td>Brad Dourif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986</td>\n",
       "      <td>The Last House on the Left</td>\n",
       "      <td>2009</td>\n",
       "      <td>Horror, Thriller</td>\n",
       "      <td>6.5</td>\n",
       "      <td>$32.75M</td>\n",
       "      <td>Dennis Iliadis</td>\n",
       "      <td>After kidnapping and brutally assaulting two y...</td>\n",
       "      <td>Garret Dillahunt,</td>\n",
       "      <td>Monica Potter,</td>\n",
       "      <td>Tony Goldwyn,</td>\n",
       "      <td>Michael Bowen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  Year                   Genre Rating  \\\n",
       "1980                   Sex Drive  2008         Comedy, Romance    6.5   \n",
       "1981                        Goon  2011    Comedy, Drama, Sport    6.8   \n",
       "1982          Skønheden i alting  2016          Drama, Romance    6.8   \n",
       "1983                    Cop Land  1997  Crime, Drama, Thriller    6.9   \n",
       "1984                    Creed II  2018            Drama, Sport    7.2   \n",
       "1985         Mississippi burning  1988   Crime, Drama, History    7.8   \n",
       "1986  The Last House on the Left  2009        Horror, Thriller    6.5   \n",
       "\n",
       "         Gross          Director  \\\n",
       "1980    $8.40M       Sean Anders   \n",
       "1981    $4.17M     Michael Dowse   \n",
       "1982   $30.98M     David Frankel   \n",
       "1983   $44.89M     James Mangold   \n",
       "1984  $115.72M  Steven Caple Jr.   \n",
       "1985   $34.60M       Alan Parker   \n",
       "1986   $32.75M    Dennis Iliadis   \n",
       "\n",
       "                                                Summary  \\\n",
       "1980  A high school senior drives cross-country with...   \n",
       "1981  Labeled an outcast by his brainy family, a bou...   \n",
       "1982  Retreating from life after a tragedy, a man qu...   \n",
       "1983  The Sheriff of a suburban New Jersey community...   \n",
       "1984  Under the tutelage of Rocky Balboa, newly crow...   \n",
       "1985  Two F.B.I. Agents, with wildly different style...   \n",
       "1986  After kidnapping and brutally assaulting two y...   \n",
       "\n",
       "                       Lead                Star 2               Star 3  \\\n",
       "1980       Josh Zuckerman,           Clark Duke,         Amanda Crew,    \n",
       "1981  Seann William Scott,         Jay Baruchel,         Alison Pill,    \n",
       "1982           Will Smith,        Edward Norton,        Kate Winslet,    \n",
       "1983   Sylvester Stallone,        Harvey Keitel,          Ray Liotta,    \n",
       "1984    Michael B. Jordan,   Sylvester Stallone,      Tessa Thompson,    \n",
       "1985         Gene Hackman,         Willem Dafoe,   Frances McDormand,    \n",
       "1986     Garret Dillahunt,        Monica Potter,        Tony Goldwyn,    \n",
       "\n",
       "               Star 4  \n",
       "1980    James Marsden  \n",
       "1981      Eugene Levy  \n",
       "1982     Michael Peña  \n",
       "1983   Robert De Niro  \n",
       "1984  Phylicia Rashad  \n",
       "1985      Brad Dourif  \n",
       "1986    Michael Bowen  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done[1980:1987]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# STARS\n",
    "done['Lead']=done['Lead'].str.replace(',','')\n",
    "done['Star 2']=done['Star 2'].str.replace(',','')\n",
    "done['Star 3']=done['Star 3'].str.replace(',','')\n",
    "# GROSS\n",
    "done[\"Gross\"] = done[\"Gross\"].str.replace(\"M\",\"\")\n",
    "done[\"Gross\"] = done[\"Gross\"].str.replace(\"$\",\"\")\n",
    "done[\"Gross\"] = done['Gross']\n",
    "# GENRE\n",
    "done[\"Genre\"] = pd.Series(done[\"Genre\"])\n",
    "new1 = done[\"Genre\"].str.split(\",\", expand=True)\n",
    "done[\"Genre 1\"]= new1[0] \n",
    "done[\"Genre 2\"]= new1[1]\n",
    "done[\"Genre 3\"]= new1[2]\n",
    "# DIRECTOR\n",
    "for i in range(10000):\n",
    "    if type(done['Director'][i]) != str:\n",
    "        done['Director'][i]=str(done['Director'][i])\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gendering the actors & director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting whether a male or female name appears\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "def preprocessing(string):\n",
    "    token=tokenizer.tokenize(string)\n",
    "    return token\n",
    "def count_dictionary(dic,doc):\n",
    "    doc_new=[ x for x in doc if x in dic]\n",
    "    return len(doc_new)\n",
    "\n",
    "done['m_count']=done['Lead'].apply(preprocessing).apply(count_dictionary, doc=sort_m)\n",
    "done['f_count']=done['Lead'].apply(preprocessing).apply(count_dictionary, doc=sort_f)\n",
    "done['star_m']=done['Lead'].apply(count_dictionary, doc=full_m)\n",
    "done['star_f']=done['Lead'].apply(count_dictionary, doc=full_f)\n",
    "done['d_m_count']=done['Director'].apply(preprocessing).apply(count_dictionary, doc=sort_m)\n",
    "done['d_f_count']=done['Director'].apply(preprocessing).apply(count_dictionary, doc=sort_f)\n",
    "done['d_star_m']=done['Director'].apply(count_dictionary, doc=male_name_director)\n",
    "done['d_star_f']=done['Director'].apply(count_dictionary, doc=female_name_director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring no properly gendered person is Neutral, and identify non-assigned. Stars given true gender\n",
    "# Gender-matching, actor\n",
    "done.loc[(done.m_count > 0), 'Male'] = 1\n",
    "done.loc[(done.f_count > 0), 'Female'] = 1\n",
    "done.loc[(done.f_count > 0) & (done.m_count > 0), 'Neutral'] = 1\n",
    "done.loc[(done.star_m) ==1, 'Neutral'] = 0\n",
    "done.loc[(done.star_m) ==1, 'Female'] = 0\n",
    "done.loc[(done.star_m) ==1, 'Male'] = 1\n",
    "done.loc[(done.star_f) ==1, 'Neutral'] = 0\n",
    "done.loc[(done.star_f) ==1, 'Male'] = 0\n",
    "done.loc[(done.star_f) ==1, 'Female'] = 1\n",
    "done.loc[(done.m_count==0) & (done.f_count==0) & (done.star_m==0) & (done.star_f==0), 'Non'] = 1\n",
    "done.loc[done.Neutral==1, 'Male'] = 0\n",
    "done.loc[done.Neutral==1, 'Female'] = 0\n",
    "\n",
    "\n",
    "# Gender-matching, director\n",
    "done.loc[(done.d_m_count > 0), 'd_Male'] = 1\n",
    "done.loc[(done.d_f_count > 0), 'd_Female'] = 1\n",
    "done.loc[(done.d_f_count > 0) & (done.d_m_count > 0), 'd_Neutral'] = 1\n",
    "done.loc[(done.d_star_m) ==1, 'd_Neutral'] = 0\n",
    "done.loc[(done.d_star_m) ==1, 'd_Female'] = 0\n",
    "done.loc[(done.d_star_m) ==1, 'd_Male'] = 1\n",
    "done.loc[(done.d_star_f) ==1, 'd_Neutral'] = 0\n",
    "done.loc[(done.d_star_f) ==1, 'd_Male'] = 0\n",
    "done.loc[(done.d_star_f) ==1, 'd_Female'] = 1\n",
    "done.loc[done.d_Neutral==1, 'd_Male'] = 0\n",
    "done.loc[done.d_Neutral==1, 'd_Female'] = 0\n",
    "done.loc[(done.d_m_count==0) & (done.d_f_count==0) & (done.d_star_m==0) & (done.d_star_f==0), 'd_Non'] = 1\n",
    "done.loc[(done.d_Neutral==1) | (done.d_Non==1), 'd_Drop'] = 1 # Create variable so that non-gendered directors can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Remove Neutral and Non and clean up\n",
    "done_1=done.drop(columns=['star_f','star_m','m_count','f_count','d_m_count','d_f_count','d_star_m','d_star_f'])\n",
    "done_1.head(1)\n",
    "done_1=done_1[done_1.Neutral != 1]\n",
    "done_1=done_1[done_1.Non != 1]\n",
    "done_1=done_1.reset_index()\n",
    "\n",
    "\n",
    "done_true=done_1.drop(columns=['index','Male','Neutral','Non','d_Male','d_Neutral','d_Non','Star 2','Star 3','Star 4'])\n",
    "done_true['Female'].fillna(0, inplace= True)\n",
    "done_true['d_Female'].fillna(0, inplace= True)\n",
    "done_true['d_Drop'].fillna(0, inplace= True)\n",
    "\n",
    "# DIRECTOR CLEAN\n",
    "\n",
    "for e in range(8262):\n",
    "    if \"[\" in done_true[\"Director\"][e]:\n",
    "        done_true[\"Director\"][e] = done_true[\"Director\"][e].split(\",\")[0].replace(\"[\",\"\").replace(\"'\",\"\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "abs_path='C:/Users/frede/OneDrive/Documents/GitHub/Group/Group-31-SoDa/Data/test.csv'\n",
    "done_true.to_csv(abs_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
