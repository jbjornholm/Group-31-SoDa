{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**<center>Text as Data</center>**\n",
    "***<center>Basic Text Classification</center>***\n",
    "\n",
    "<center>Snorre Ralund</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "** Quantifying the Qualitative **  \n",
    "- **Behavior** (ethnography) instead of **Attitudes**(Survey)\n",
    "- Dynamics of **Ideas** and Forms (Historical analysis) instead of rationalized **Answers** (Interview) and **Numbers** (Register).\n",
    "- Social relationships and their content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples\n",
    "- Measuring historical development of jobdemand from jobpostings.\n",
    "- Opportunism in politics:\n",
    "    - Shifting political topics based on Response.\n",
    "- Ethnic and Political Polarization through expressed social ties.\n",
    "    - Processing the social signals in textual social media content.\n",
    "    - Respect and Hostility towards the Stranger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview of Computational Content Analysis\n",
    "![](computational_content_analysis_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "** Natural Language Processing basics - Representation and Classification **\n",
    "- Baseline classifiers using the BoW representation.\n",
    "    - Tokenization\n",
    "    - Stemming and Lemmatization\n",
    "    - Ngram and Collocations.\n",
    "- Lexical and rulebased approaches to classification\n",
    "\n",
    "** Computational Content Analysis Methodology ** \n",
    "- Natural language processing as a ***Measurement device***.\n",
    "    - Validation of automated procedure.\n",
    "- Understanding and Accounting for the ***Measured Category***\n",
    "    - Computationally grounded qualitative analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Text Classification\n",
    "![](stanford_pajamas_constituency_parsing.png)\n",
    "\n",
    "- Language Understanding is hard, but... \n",
    "    - Many useful classification tasks just need simple recognition of keywords.\n",
    "    - Many interesting questions can come from simple statistical descriptions.\n",
    "        - lengths of sentences or documents\n",
    "        - lix and wordlength\n",
    "        - the use of ALLCAPS, commas and punctuation (,!?;:.\"\").\n",
    "        - the use of emoticons/emojies\n",
    "        - $ â‚¬ kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"... be prepared to spent 30-40 minutes fighting with the unpacking, and cutting up a HUGE shipping box!!!!  I'M SERIOUS!!!  I recently ordered a frame (12x36) with matt board and plexiglass (not glass).  My wife helped me carry a HUGE box into the house, and after about half an hour of cutting through layers of very thick cardboard, the unpacking was finally complete and we had a HUGE pile of cut up cardboard and packing material... what an incredible waste!!  I contacted them and sent pictures.  I told them to please stop this unnecessary overkill... they ignored my suggestion.  I just received a smaller order (11x14) and you would think I ordered quantity 10 by the size of the box!!!  LOVE their products, I am tired of hauling piles of cardboard to the recycle center because there is so much with each order it will not fit in my recycle bin!!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple description example.\n",
    "### Download data\n",
    "import requests,pandas as pd,numpy as np\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)\n",
    "sentence = df.iloc[3267].reviewBody\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n",
      "12\n",
      "14\n",
      "4.629139072847682\n"
     ]
    }
   ],
   "source": [
    "# ALLCAPS\n",
    "print('YEAH'.isupper(),'yeah'.islower())\n",
    "upper = 0\n",
    "for w in sentence.split(): # naive tokenization\n",
    "    upper+=w.isupper()\n",
    "print(upper)\n",
    "#Counting simple pattern\n",
    "print(sentence.count('!'))\n",
    "# wordlengths\n",
    "words = sentence.split() # naive tokenization\n",
    "lengths = [len(w) for w in words]\n",
    "print(np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Classification (2) - BoW Baseline\n",
    "### Getting from Text to vector\n",
    "![](https://image.slidesharecdn.com/bcw-cochrane-tech-2013-130927075508-phpapp02/95/text-mining-machine-learning-nlp-and-all-that-in-10-minutes-10-638.jpg?cb=1380268595)\n",
    "<center>Credit Byron C Wallace</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector representation \n",
    "## Bag of Words (BoW) or Document Term Matrix\n",
    "* Throw out the word order.\n",
    "* And let each word be a feature. --> map word to an index in a matrix.\n",
    "\n",
    "\n",
    "doc1: \"i really love bacon\"\n",
    "\n",
    "doc2: \"i really don't like bacon\"\n",
    "\n",
    "<center> **As a Document Term Matrix** <center>\n",
    "\n",
    "document | really | i | love | bacon | don't\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "*doc1* | 0 | 0 | 1 | 1 | 0 |\n",
    "*doc2* | 1 | 1 | 0 | 1 | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1200/1*unP1qDkUPSSUsa2-530zEg.png)\n",
    "- Baseline models: Naive Bayes, Logistic Regresion, K-nearest Neighbor and Support Vector Machines\n",
    "- **Wang and Manning 2012** *\"Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\"*: \n",
    "    - State-of-the-art (2012) Topic and Sentiment Classification using only atomized Words as input(BoW) to a linear model. \n",
    "    - No grammar or reasoning.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of Words(2) - Problem with polesemy\n",
    "\n",
    "Consider the following to documents: \n",
    "\n",
    "doc1: *\"River A/S declared default by the bank.\"*\n",
    "\n",
    "doc2: *\"When camping my default is by the river bank.\"*\n",
    "\n",
    "<center> **As a Document Term Matrix** <center>\n",
    "\n",
    "document | declared | by | default | bank | river | a/s | when | camping | my\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
    "*doc1* | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 \n",
    "*doc2* | 0 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of Words (3) - Lack of word order\n",
    "doc1 = 'this was not the best movie'\n",
    "\n",
    "doc2 = 'was this not the best movie ever?'\n",
    "\n",
    "Will have very similar representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ngrams to the Rescue\n",
    "#### Word Ngram \n",
    "![](https://i.stack.imgur.com/8ARA1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Ngrams to the Rescue(2)\n",
    "** Problem with dimensionality ** \n",
    "Quad-grams Qvint Grams etc. Generates exponential number of features.\n",
    "\n",
    "*** Solution *** Pick only the ngrams using statistical analysis of the word co-occurences.\n",
    "Check out methods for doing so in the Natural Language Processing Toolkit package nltk: `nltk.collocations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenization: From string to words to concepts\n",
    "![](https://www.incimages.com/uploaded_files/image/970x450/getty_850704072_375370.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'are', 'the', 'boundaries', 'between-words', 'and', 'meaning?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"What are the boundaries between-words and meaning?\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WORDS (1)\n",
    "How we \"split\" / locate words in text determines the number of dimensions (columns in the Document-Term Matrix)\n",
    "* Computational inefficiency \n",
    "* Parameters are not shared among equivalent words.\n",
    "    * It makes a difference especially for low N tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Words (2)\n",
    "**ISSUES**\n",
    "* Spelling mistakes, weird uses of punctuation,\n",
    "* Emoticons: </3 , (:) , :-]\n",
    "* Multiwords: #no-more-work, New York, Federal Bureau of Finance, word/concept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation (1) \n",
    "**How to encode all relevant information in our tokens?**\n",
    "* lower-casing: DO YOU REALLY WANT TO IGNORE MY ALLCAPS?!?!\n",
    "    * Our featurespace can potentially double if we don't lowercase.\n",
    "* Numbers: Infinite combinations of digits\n",
    "* Filtering to reduce dimensions: Which words to lose?\n",
    "    * Common or Rare?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation (2) \n",
    "**Grammatical Forms: Do we need grammar?**\n",
    "- Stemming\n",
    "    - Rulebased: Strips common endings: 'ing','ly','s'\n",
    "- Lemmatization\n",
    "    - Lookup in Lexical Ressources: ran --> run, running --> run\n",
    "\n",
    "** Trade-off precision and coverage **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WORDS (4) - choosing a tokenizer and preprocessing scheme\n",
    "** There are many tokenizers: which one to choose depends on task **\n",
    "- no. of samples (lemmatization and stemming)\n",
    "- how much subtlety do you need\n",
    "     - e.g. ['\"truth\"'] or ['\"' ,'truth','\"']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizer Experiment (1) - which is best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](tokenizer_similarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KlatretÂ©Âªsen(Catch That Girl) is really great movie! It's a 'happy' movie. I watched this movie in 'Puchon International Fantastic Film Festival(PiFan)' on July 12nd, 2003. There is Action + Adventure + Comedy + Thrill + Happy + Romance(cute kids' love Triangle!). You must see this movie. :)\n"
     ]
    }
   ],
   "source": [
    "print(\"KlatretÂ©Âªsen(Catch That Girl) is really great movie! It's a 'happy' movie. I watched this movie in 'Puchon International Fantastic Film Festival(PiFan)' on July 12nd, 2003. There is Action + Adventure + Comedy + Thrill + Happy + Romance(cute kids' love Triangle!). You must see this movie. :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](cv_tokenizer_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WORDS (4) - choosing a tokenizer and preprocessing scheme\n",
    "For user generated content and social media data use:\n",
    "- If enough data: nltk.tokenize.causal.tweet \n",
    "- If smaller data: spacy lemmatizer.\n",
    "\n",
    "For more formal text (e.g. scientific articles) test a few others.\n",
    "<pre><code> \n",
    "import nltk\n",
    "tweet_tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "tweet_tokenizer.tokenize('hello I speak emoticon and #hashtag :)'\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexical approaches to text mining\n",
    "** Automatic classification based on Rules and Lookups**\n",
    "\n",
    "**Pros**\n",
    "* Comes with very low cost. \n",
    "* Fast and scalable.\n",
    "* Good for prototyping results.\n",
    "\n",
    "* Good for certain tasks (e.g. topic classification)\n",
    "\n",
    "** Example ** \n",
    "- 300 million documents, more than 5 million unique tokens. How to inquire?\n",
    "\n",
    "![](https://raw.githubusercontent.com/snorreralund/sds_dump/master/toneDK.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precompiled Lexicons\n",
    "**Sentiment**\n",
    "* [Afinn (is danish!)](http://neuro.imm.dtu.dk/wiki/AFINN), \n",
    "* [Liu Hu (lexical)](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) and \n",
    "* [Vader (lexical and rulebased)](https://github.com/cjhutto/vaderSentiment).\n",
    "\n",
    "\n",
    "* ** Purely Lexical ** Naively Matching positive words. *\"You are beautiful.\"* \n",
    "* ** Rule-based ** Can Adopt hard-coded rules to counter more or less simple negations. *\"You are not particularly beautiful.\"*\n",
    "\n",
    "** See notebook [lexical_mining.ipynb](https://github.com/abjer/tsds/blob/master/material/12_text2/lexical_mining.py) for a compilation of lexicon classifiers. ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### issues with the lexical-based approach (1)\n",
    "\n",
    "*\"Pretty. Pretty actresses and actors. Pretty bad script. Pretty frequent \"let's strip to our undies\" scenes. Pretty fair F/X. Pretty jarring location decisions (the college dorm room looks like a high-end hotel room - probably because it was shot at a hotel). Pretty bland storyline. Pretty awful dialog. Pretty locations. Pretty annoying editing, unless you like the music video flash-cut style.This one isn't a guilty pleasure - this is more an embarrassing one. If you must watch this, pick a good dance/techno album and turn the sound off on the movie - you'll see the pretty people in their pretty black undies, and probably follow the story just fine.The cast may be able to act - I doubt that anyone could look skilled given the lines/plot that they had to deal with.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### issues with the lexical-based approach (2)\n",
    "- Atomized words: How well can meaning be derived from atomized words?  \n",
    "     - Not applicable to more complex rulebased versions:\n",
    "         - e.g. VADER\n",
    "         - Argument dictionary phrase based.\n",
    "       - Use in connection with Parsers: e.g. who is the emotion directed. \n",
    "- What is the Recall?\n",
    "    - Bad practice using dictionaries without explicit validation.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### use for prototyping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### issues with the lexical-based approach (3)\n",
    "**Conglomerates of words = Concept?**\n",
    "- What is the theoretical validity of a collection of words, scraped from many sources, validated at some historical point in time, given some score by a number people (students, amazonturks)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datadriven discovery and generation of lexicons\n",
    "- Efficient generation of your own lexicons using an active learning approach [(King, Lam and Robert 2017)](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text) and word similarity search [(e.g. Marquez et al. 2016)](https://www.cs.waikato.ac.nz/~eibe/pubs/emo_lex_wi.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python Ressources for Text as Data\n",
    "- `nltk`: General purpose and educational natural language processing library.\n",
    "    - Includes many tokenizers, stemmers and a collocation module. \n",
    "- `spacy`: For more advanced natural language processing pipeline.\n",
    "- `stanfordnlp`: For state-of-the-art dependency parsing in more than 50 languages. \n",
    "    - Parsing of grammar, named-entity-extraction, subject-verb-object.\n",
    "- `gensim`: Unsupervised learning and Clustering of text\n",
    "    - Including Topic Modelling implementations, and Word2Vec style word-similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methodological Considerations - (i.e. EXAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Content Analysis (1)\n",
    "![](https://matlab1.com/wp-content/uploads/2017/11/tolomerok-kalibralasa-tevekenysegek.jpg)\n",
    "- NLP is a **Measurement device**\n",
    "    - How well is it calibrated?\n",
    "    - Does it have any biases that might be relevant to the results produced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For the Exam\n",
    "**Discuss potential biases in classification, and their impact on results.**\n",
    "-Issues could be: \n",
    "    - Missing keywords using the lexical approach\n",
    "    - Sarcasm not equally distributed accross social classes.\n",
    "     \n",
    "- Take a ***small*** random sample of the models predictions for annotation (multiple coders). \n",
    "    - Report intercoder-agreement (http://www.nltk.org/api/nltk.metrics.html) and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Content Analysis (2)\n",
    "![](https://systweak1.vo.llnwd.net/content/wp/systweakblogsnew/uploads_new/2018/04/98993-610x377-Why-People-Dont.jpg)\n",
    "- Before we trust a **Machine**, we need to trust the **Instructor** first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Construct Validity in (Automated) Content Analysis (2)\n",
    "Krippendorf 2004: *\"Content Analysis\"*\n",
    "\n",
    "- When constructing a variable from an interpretation of text (as opposed to survey, or registerbased research).\n",
    "    - We have the *possibility*,\n",
    "    - and the *obligation* to show that our interpretation are **correct**.\n",
    "    - be **precise**.\n",
    "        - What is it that we are measuring?\n",
    "        - What are we not measuring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trusting the Instructor (Exam)\n",
    "- A clear definition (and delimitation) of the Category one wish to measure.\n",
    "    - Give examples from the data.\n",
    "    - Discuss the potential challenges involved in the automatization of classification\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
