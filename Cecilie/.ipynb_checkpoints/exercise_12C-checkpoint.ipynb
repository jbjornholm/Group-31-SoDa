{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 12: Linear regression models.\n",
    "\n",
    "*Afternoon, August 19, 2019*\n",
    "\n",
    "In this Exercise Set 12 we will work with linear regression models.\n",
    "\n",
    "We import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.1: Estimating linear models with gradient decent\n",
    " \n",
    "Normally we use OLS to estimate linear models. In this exercise we replace the OLS-estimator with a new estimator that we code up from scratch. We solve the numerical optimization using the gradient decent algorithm. Using our algorithm we will fit it to some data, and compare our own solution to the standard solution from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.0**: Import the dataset `tips` from the `seaborn`.\n",
    "\n",
    "\n",
    "*Hint*: use the `load_dataset` method in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.29</td>\n",
       "      <td>4.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.77</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.88</td>\n",
       "      <td>3.12</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.04</td>\n",
       "      <td>1.96</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.78</td>\n",
       "      <td>3.23</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.27</td>\n",
       "      <td>1.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35.26</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.42</td>\n",
       "      <td>1.57</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.43</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.83</td>\n",
       "      <td>3.02</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.58</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.29</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16.97</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.65</td>\n",
       "      <td>3.35</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.92</td>\n",
       "      <td>4.08</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20.29</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.77</td>\n",
       "      <td>2.23</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39.42</td>\n",
       "      <td>7.58</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.82</td>\n",
       "      <td>3.18</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.81</td>\n",
       "      <td>2.34</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.37</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.69</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>21.70</td>\n",
       "      <td>4.30</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>28.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>12.90</td>\n",
       "      <td>1.10</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>28.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>11.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>7.74</td>\n",
       "      <td>1.44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>30.14</td>\n",
       "      <td>3.09</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>12.16</td>\n",
       "      <td>2.20</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13.42</td>\n",
       "      <td>3.48</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>8.58</td>\n",
       "      <td>1.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>15.98</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>13.42</td>\n",
       "      <td>1.58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>16.27</td>\n",
       "      <td>2.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>10.09</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>20.45</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>13.28</td>\n",
       "      <td>2.72</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>22.12</td>\n",
       "      <td>2.88</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>24.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>15.69</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>11.61</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>10.77</td>\n",
       "      <td>1.47</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>15.53</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>10.07</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>12.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>32.83</td>\n",
       "      <td>1.17</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>35.83</td>\n",
       "      <td>4.67</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip     sex smoker   day    time  size\n",
       "0         16.99  1.01  Female     No   Sun  Dinner     2\n",
       "1         10.34  1.66    Male     No   Sun  Dinner     3\n",
       "2         21.01  3.50    Male     No   Sun  Dinner     3\n",
       "3         23.68  3.31    Male     No   Sun  Dinner     2\n",
       "4         24.59  3.61  Female     No   Sun  Dinner     4\n",
       "5         25.29  4.71    Male     No   Sun  Dinner     4\n",
       "6          8.77  2.00    Male     No   Sun  Dinner     2\n",
       "7         26.88  3.12    Male     No   Sun  Dinner     4\n",
       "8         15.04  1.96    Male     No   Sun  Dinner     2\n",
       "9         14.78  3.23    Male     No   Sun  Dinner     2\n",
       "10        10.27  1.71    Male     No   Sun  Dinner     2\n",
       "11        35.26  5.00  Female     No   Sun  Dinner     4\n",
       "12        15.42  1.57    Male     No   Sun  Dinner     2\n",
       "13        18.43  3.00    Male     No   Sun  Dinner     4\n",
       "14        14.83  3.02  Female     No   Sun  Dinner     2\n",
       "15        21.58  3.92    Male     No   Sun  Dinner     2\n",
       "16        10.33  1.67  Female     No   Sun  Dinner     3\n",
       "17        16.29  3.71    Male     No   Sun  Dinner     3\n",
       "18        16.97  3.50  Female     No   Sun  Dinner     3\n",
       "19        20.65  3.35    Male     No   Sat  Dinner     3\n",
       "20        17.92  4.08    Male     No   Sat  Dinner     2\n",
       "21        20.29  2.75  Female     No   Sat  Dinner     2\n",
       "22        15.77  2.23  Female     No   Sat  Dinner     2\n",
       "23        39.42  7.58    Male     No   Sat  Dinner     4\n",
       "24        19.82  3.18    Male     No   Sat  Dinner     2\n",
       "25        17.81  2.34    Male     No   Sat  Dinner     4\n",
       "26        13.37  2.00    Male     No   Sat  Dinner     2\n",
       "27        12.69  2.00    Male     No   Sat  Dinner     2\n",
       "28        21.70  4.30    Male     No   Sat  Dinner     2\n",
       "29        19.65  3.00  Female     No   Sat  Dinner     2\n",
       "..          ...   ...     ...    ...   ...     ...   ...\n",
       "214       28.17  6.50  Female    Yes   Sat  Dinner     3\n",
       "215       12.90  1.10  Female    Yes   Sat  Dinner     2\n",
       "216       28.15  3.00    Male    Yes   Sat  Dinner     5\n",
       "217       11.59  1.50    Male    Yes   Sat  Dinner     2\n",
       "218        7.74  1.44    Male    Yes   Sat  Dinner     2\n",
       "219       30.14  3.09  Female    Yes   Sat  Dinner     4\n",
       "220       12.16  2.20    Male    Yes   Fri   Lunch     2\n",
       "221       13.42  3.48  Female    Yes   Fri   Lunch     2\n",
       "222        8.58  1.92    Male    Yes   Fri   Lunch     1\n",
       "223       15.98  3.00  Female     No   Fri   Lunch     3\n",
       "224       13.42  1.58    Male    Yes   Fri   Lunch     2\n",
       "225       16.27  2.50  Female    Yes   Fri   Lunch     2\n",
       "226       10.09  2.00  Female    Yes   Fri   Lunch     2\n",
       "227       20.45  3.00    Male     No   Sat  Dinner     4\n",
       "228       13.28  2.72    Male     No   Sat  Dinner     2\n",
       "229       22.12  2.88  Female    Yes   Sat  Dinner     2\n",
       "230       24.01  2.00    Male    Yes   Sat  Dinner     4\n",
       "231       15.69  3.00    Male    Yes   Sat  Dinner     3\n",
       "232       11.61  3.39    Male     No   Sat  Dinner     2\n",
       "233       10.77  1.47    Male     No   Sat  Dinner     2\n",
       "234       15.53  3.00    Male    Yes   Sat  Dinner     2\n",
       "235       10.07  1.25    Male     No   Sat  Dinner     2\n",
       "236       12.60  1.00    Male    Yes   Sat  Dinner     2\n",
       "237       32.83  1.17    Male    Yes   Sat  Dinner     2\n",
       "238       35.83  4.67  Female     No   Sat  Dinner     3\n",
       "239       29.03  5.92    Male     No   Sat  Dinner     3\n",
       "240       27.18  2.00  Female    Yes   Sat  Dinner     2\n",
       "241       22.67  2.00    Male    Yes   Sat  Dinner     2\n",
       "242       17.82  1.75    Male     No   Sat  Dinner     2\n",
       "243       18.78  3.00  Female     No  Thur  Dinner     2\n",
       "\n",
       "[244 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.1.0]\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.1**: Convert non-numeric variables to dummy variables for each category (remember to leave one column out for each catagorical variable, so you have a reference). Restructure the data so we get a dataset `y` containing the variable tip, and a dataset `X` containing the \n",
    "features. \n",
    "\n",
    ">> *Hint*: You might want to use the `get_dummies` method in pandas, with the `drop_first = True` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.1]\n",
    "#dummy_list = ['sex', 'smoker', 'day', 'time']\n",
    "tips_data= pd.get_dummies(tips, columns=['sex', 'smoker', 'time', 'day'], drop_first = True)\n",
    "\n",
    "y = tips_data['tip']\n",
    "X = tips_data.drop('tip', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.2**: Divide the features and target into test and train data. Make the split 50 pct. of each. The split data should be called `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    ">> *Hint*: You may use `train_test_split` in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.2]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.3**: Normalize your features by converting to zero mean and one std. deviation.\n",
    "\n",
    ">> *Hint 1*: Take a look at `StandardScaler` in `sklearn.preprocessing`. \n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.3]\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "\n",
    "norm_scaler = StandardScaler().fit(X_train)\n",
    "X_train = norm_scaler.transform(X_train)\n",
    "X_test = norm_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.4**: Make a function called `compute_error` to compute the prediction errors given input target `y_`, input features `X_` and input weights `w_`. You should use matrix multiplication.\n",
    ">\n",
    ">> *Hint 1:* You can use the net-input fct. from yesterday.\n",
    ">>\n",
    ">> *Hint 2:* If you run the following code,\n",
    ">> ```python\n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "compute_error(y__, X__, w__)\n",
    "```\n",
    "\n",
    ">> then you should get output:\n",
    "```python \n",
    "array([0,0])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.4]\n",
    "def compute_error(y, X, w):\n",
    "        e = y - (w[0] + X.dot(w[1:]))\n",
    "        return e\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "compute_error(y__, X__, w__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.5**: Make a function to update the weights given input target `y_`, input features `X_` and input weights `w_` as well as learning rate, $\\eta$, i.e. greek `eta`. You should use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.5]\n",
    "def update(y, X, w, eta):    \n",
    "    for xi, yi in zip(X,y):\n",
    "        update = eta *X.T.dot(compute_error(y,X,w))\n",
    "        w[1:] = w[1:] + update * xi\n",
    "        #w[0] = w[0] + eta*compute_error(y,X,w)\n",
    "    return w\n",
    "\n",
    "update(X__, y__, w__, 0.1)\n",
    "\n",
    "def update_weight(y_, X_, w_, eta):\n",
    "    error = compute_error(y_, X_, w_)    \n",
    "    w_[1:] += eta * (X_.T.dot(error))\n",
    "    w_[0] += eta * (error).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.6**: Use the code below to initialize weights `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.6]\n",
    "w = np.zeros(1+X.shape[1])\n",
    "for i in range(50):\n",
    "    _, e = update(xtraining_dataset, ytraining_dataset, w)\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.7**: Make a function to compute the mean squared error. Alter the loop so it makes 100 iterations and computes the MSE for test and train after each iteration, plot these in one figure. \n",
    "\n",
    ">> Hint: You can use the following code to check that your model works:\n",
    ">>```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "assert((w[1:] - reg.coef_).sum() < 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bonus exercises are for those who have completed all other exercises until now and have a deep motivation for learning more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.8 (BONUS)**: Implement your linear regression model as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.9 (BONUS)**: Is it possible to adjust our linear model to become a Lasso? Is there a simple fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.2: Houseprices\n",
    "In this example we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MedInc  HouseAge   AveRooms  AveBedrms  Population  AveOccup\n",
      "10089  4.0893      35.0   5.267760   0.983607      1056.0  2.885246\n",
      "2136   3.7578      24.0   5.061538   0.957692       781.0  3.003846\n",
      "17546  2.4306      39.0   4.899209   1.069170      1990.0  3.932806\n",
      "10051  3.2813      10.0   6.030928   1.159794       537.0  2.768041\n",
      "3627   4.0950      36.0   5.407166   0.980456      1225.0  3.990228\n",
      "10862  5.0956      21.0   5.808917   1.041401      2266.0  3.608280\n",
      "10781  5.0532      36.0   5.795977   1.025862       850.0  2.442529\n",
      "6386   4.8214      34.0   5.575243   1.016990       909.0  2.206311\n",
      "13958  1.9980      27.0  21.805654   4.646643      1300.0  2.296820\n",
      "8409   1.9766      34.0   3.427778   1.050000       810.0  4.500000\n",
      "13985  1.8958      28.0   4.782178   1.042079      1156.0  2.861386\n",
      "16066  3.7734      48.0   5.300830   1.136929      1285.0  2.665975\n",
      "10250  3.7361      19.0   5.034161   1.018634       871.0  2.704969\n",
      "17666  2.7031      36.0   4.512270   1.104294      1182.0  3.625767\n",
      "6441   2.7857      30.0   5.135185   1.174074      1376.0  2.548148\n",
      "1878   2.8300      22.0   5.654244   1.308489      1215.0  2.515528\n",
      "14653  2.1157      41.0   5.199571   1.006438      1813.0  3.890558\n",
      "365    4.9750      30.0   8.280000   1.080000       137.0  2.740000\n",
      "15037  8.0930      29.0   7.995506   1.004494      1097.0  2.465169\n",
      "16823  4.2593      39.0   5.011976   0.997006      1817.0  2.720060\n",
      "16611  4.4750      13.0   6.970313   1.193750      1795.0  2.804688\n",
      "10696  3.5625      17.0   4.776524   1.069977       662.0  1.494357\n",
      "12314  3.0694       6.0   5.791667   1.240741       544.0  2.518519\n",
      "16907  4.9700      52.0   6.685065   1.120130       800.0  2.597403\n",
      "20341  4.7831      24.0   5.303876   1.017054      2220.0  3.441860\n",
      "12351  3.4394      12.0   9.799209   2.017391      2548.0  2.014229\n",
      "1351   1.4871      27.0   3.938095   1.138095      1141.0  2.716667\n",
      "3601   5.2668      32.0   6.310245   0.985570      2040.0  2.943723\n",
      "4569   2.3438      42.0   2.886978   1.051597      1593.0  3.914005\n",
      "9945   6.8000      33.0   6.539877   1.171779       403.0  2.472393\n",
      "...       ...       ...        ...        ...         ...       ...\n",
      "3606   4.5484      29.0   5.160000   1.087692      1752.0  2.695385\n",
      "19261  2.6576      23.0   5.163004   1.120879      1644.0  3.010989\n",
      "4415   2.9350      52.0   4.474820   1.127098      1211.0  2.904077\n",
      "1031   2.0500      34.0   6.058824   1.358289       495.0  2.647059\n",
      "6285   4.9342      23.0   5.735798   1.053201      4580.0  4.129847\n",
      "1110   2.6147      11.0   6.268139   1.146688      1610.0  2.539432\n",
      "18272  7.5274      37.0   7.068536   1.012461       930.0  2.897196\n",
      "11742  4.2723      23.0   5.531773   1.000000       787.0  2.632107\n",
      "17137  4.2667      32.0   3.633015   0.901774      1403.0  1.914052\n",
      "19433  3.3567       8.0   5.367123   1.038356       995.0  2.726027\n",
      "16946  6.1349      33.0   6.893417   0.978056       827.0  2.592476\n",
      "4764   3.5161      45.0   4.662269   1.100264      1052.0  2.775726\n",
      "19946  1.4429      32.0   4.174757   0.983819       852.0  2.757282\n",
      "8444   3.9355      41.0   4.296552   0.981609      1334.0  3.066667\n",
      "18900  1.9667      52.0   4.087629   1.113402       505.0  2.603093\n",
      "2962   2.6637      14.0   3.925439   1.078947      1933.0  2.119518\n",
      "12645  1.2575      26.0   3.887500   1.139286      1971.0  3.519643\n",
      "3462   4.1514      10.0   5.470644   1.053030      2917.0  2.762311\n",
      "10989  3.5719      19.0   4.387330   1.078589      2828.0  2.267843\n",
      "7751   2.5875      28.0   4.236735   1.028571       912.0  3.722449\n",
      "16332  4.4063      20.0   5.900560   1.051821      2071.0  2.900560\n",
      "20609  2.3011      18.0   4.856823   1.073826      1527.0  3.416107\n",
      "144    3.0812      38.0   4.628337   1.098563       951.0  1.952772\n",
      "19279  1.9458      26.0   4.709677   1.020161       937.0  3.778226\n",
      "7813   4.4896      35.0   5.540426   1.053191      1465.0  3.117021\n",
      "10955  1.7823      17.0   4.055046   1.087156      1079.0  2.474771\n",
      "17289  8.5608      42.0   6.788462   1.011538       753.0  2.896154\n",
      "5192   1.1326      42.0   4.890785   1.006826       775.0  2.645051\n",
      "12172  2.6322      10.0   4.991614   0.951782      1323.0  2.773585\n",
      "235    2.3036      35.0   4.620513   1.176923      1009.0  2.587179\n",
      "\n",
      "[10320 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.0]\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.874334</td>\n",
       "      <td>28.506977</td>\n",
       "      <td>5.448060</td>\n",
       "      <td>1.098334</td>\n",
       "      <td>1426.466860</td>\n",
       "      <td>3.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.875069</td>\n",
       "      <td>12.638869</td>\n",
       "      <td>2.710030</td>\n",
       "      <td>0.543761</td>\n",
       "      <td>1098.387561</td>\n",
       "      <td>7.727201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.579425</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.462767</td>\n",
       "      <td>1.006410</td>\n",
       "      <td>787.750000</td>\n",
       "      <td>2.428799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.549850</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.235723</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1162.500000</td>\n",
       "      <td>2.822316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.736450</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.070853</td>\n",
       "      <td>1.098592</td>\n",
       "      <td>1726.250000</td>\n",
       "      <td>3.281516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>16305.000000</td>\n",
       "      <td>599.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  10320.000000  10320.000000  10320.000000  10320.000000  10320.000000   \n",
       "mean       3.874334     28.506977      5.448060      1.098334   1426.466860   \n",
       "std        1.875069     12.638869      2.710030      0.543761   1098.387561   \n",
       "min        0.499900      1.000000      0.888889      0.333333      5.000000   \n",
       "25%        2.579425     18.000000      4.462767      1.006410    787.750000   \n",
       "50%        3.549850     29.000000      5.235723      1.048780   1162.500000   \n",
       "75%        4.736450     37.000000      6.070853      1.098592   1726.250000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  16305.000000   \n",
       "\n",
       "           AveOccup  \n",
       "count  10320.000000  \n",
       "mean       3.046432  \n",
       "std        7.727201  \n",
       "min        1.060606  \n",
       "25%        2.428799  \n",
       "50%        2.822316  \n",
       "75%        3.281516  \n",
       "max      599.714286  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect for missing data\n",
    "\n",
    "X_train.isnull()\n",
    "X.describe()\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex.12.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    "\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    "\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33298175,  0.83025875, -0.34866789, ..., -0.04726926,\n",
       "        -0.01443505, -0.01349305],\n",
       "       [-1.01117619,  0.67200908, -0.16955442, ..., -0.04210401,\n",
       "        -0.01424053, -0.01348811],\n",
       "       [ 0.07406292,  1.38413261, -0.35712192, ..., -0.01896188,\n",
       "        -0.01359914, -0.01347342],\n",
       "       ...,\n",
       "       [ 0.44596052,  0.98850843, -0.43205052, ..., -0.04983522,\n",
       "        -0.01445712, -0.01349298],\n",
       "       [ 0.30643892, -0.98961249,  0.1913317 , ..., -0.02163071,\n",
       "        -0.01393352, -0.01348493],\n",
       "       [ 1.011247  , -0.91048766,  0.10956687, ..., -0.03399015,\n",
       "        -0.01428989, -0.01349191]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.1]\n",
    "\n",
    "X_train_p = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X_train) \n",
    "X_test_p = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X_test) \n",
    "\n",
    "\n",
    "norm_scaler = StandardScaler()\n",
    "X_train_scale = norm_scaler.fit_transform(X_train_p)\n",
    "X_test_scale = norm_scaler.transform(X_test_p)\n",
    "\n",
    "X_test_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.2**: Estimate the Lasso model on the train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the test and train data. \n",
    "\n",
    "> *Hint*: use `logspace` in numpy to create the range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.2]\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from math import sqrt\n",
    "#rms = sqrt(mse(y_actual, y_predicted))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "perform_test = []\n",
    "perform_train = []\n",
    "\n",
    "lambdas = np.logspace(-4, 4, 17)\n",
    "for lambda_ in lambdas:\n",
    "    lasso = Lasso(alpha=lambda_, random_state=1)\n",
    "    lasso.fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = lasso.predict(X_test_scale)\n",
    "    y_pred_train = lasso.predict(X_train_scale)\n",
    "    perform_test.append(sqrt(mse(y_pred, y_test)))\n",
    "    perform_train.append(sqrt(mse(y_pred_train, y_train)))\n",
    "    \n",
    "#hyperparam_perform = pd.Series(perform,index=lambdas)\n",
    "\n",
    "done = pd.DataFrame({'lambda':lambdas, 'train_mse': perform_train, 'test_mse':perform_test})\n",
    "\n",
    "\n",
    "#optimal = hyperparam_perform.nsmallest(1)    \n",
    "#print('Optimal lambda:', optimal.index[0])\n",
    "#print('Validation MSE: %.3f' % optimal.values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.3**: Make a plot with on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b634eadba8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.3]\n",
    "\n",
    "done.plot(x = 'lambda', logx=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
