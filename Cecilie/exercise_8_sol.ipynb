{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Connector class for accessing the internet\n",
    "Even if logging is not important for the below exercises, get in the habit of using this class for connecting to the internet, to practice logging your activity. This will be expected in the final exam.\n",
    "\n",
    "You should run `pip install scraping_class` to install the module to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 8: Introduction to Web Scraping\n",
    "\n",
    "*Afternoon, August 15, 2019*\n",
    "\n",
    "In this Exercise Set we shall practice our webscraping skills utiilizing only basic python. We shall cover variations between static and dynamic pages and build. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.1: Scraping Jobnet.dk\n",
    "\n",
    "This exercise you get to practice locating the request that the JavaScript sends to get the job data that it builds the joblistings from. You should use the **>Network Monitor<** tool in your browser.\n",
    "\n",
    "Furthermore you practice spotting how the pagination is done, without clicking on the next page button, but instead changing a small parameter in the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1:** Hit the joblisting webpage here: https://job.jobnet.dk/CV and locate the request that gets the joblisting data using the the **>Network Monitor<**. *(Hint: Filter by XHR files)  \n",
    "\n",
    "> **Ex. 8.1.2.:** Use the `request` module to collect the first 20 results and unpack the relevant `json` data into a `pandas` DataFrame.\n",
    "\n",
    "> **Ex. 8.1.3.:** Store the 'TotalResultCount' value for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.1-3 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://job.jobnet.dk/CV/FindWork/Search?SortValue=CreationDate&Offset=0'\n",
    "response,call_id = connector.get(url,'mapping_jobposting')\n",
    "if response.ok:\n",
    "    d = response.json()\n",
    "else:\n",
    "    print('error')\n",
    "df = pd.DataFrame(d['JobPositionPostings'])\n",
    "df.head()\n",
    "n_listings = d['TotalResultCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.4:** This exercise is about paging the results. We need to understand the websites pagination scheme. \n",
    "\n",
    "> Now scroll down the webpage and press the next page button. See how the parameters of the url changes as you turn the pages.\n",
    "\n",
    "> **Ex. 8.1.5:** Design a`for` loop using the `range` function that changes this paging parameter in the URL. Use the TotalResultCount parameter from before to define the limits of the range function. Store these urls in a container. \n",
    "\n",
    ">**extra** Change the SortValue parameter from BestMatch to CreationDate, to make the sorting amendable to updating results daily.\n",
    "\n",
    "*(HINT: See that the parameter is an offset and that this relates to the number of results pr. call made.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.4-5 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 'https://job.jobnet.dk/CV/FindWork/Search?SortValue=CreationDate&Offset=%d'\n",
    "links = []\n",
    "for offset in range(0,n_listings+20,20):\n",
    "    url = q%offset\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.8.1.6:** Pick 20 random links using the `random.sample()` function and collect them using the `Connector` class. Also use the `time.sleep()` function to limit the rate of your calls. Make sure to save the links already collected in a `set()` container to avoid having to reload links already collected. ***extra***: monitor the time left to completing the loop by using `tqdm.tqdm()` function.\n",
    "\n",
    "> **Ex.8.1.7:** Load all the results into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.6-7 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "done = set()\n",
    "data = []\n",
    "import tqdm\n",
    "for url in tqdm.tqdm(random.sample(links,20)):\n",
    "    response,call_id = connector.get(url,'download_jobposting')\n",
    "    if response.ok:\n",
    "        d = response.json()\n",
    "    else:\n",
    "        print('error')\n",
    "    data += d['JobPositionPostings']\n",
    "    time.sleep(0.5)\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.2: Scraping Trustpilot.com\n",
    "Now for a slightly more elaborate, yet still simple scraping problem. Here we want to scrape trustpilot for user reviews. This data is very nice since it provides free labeled data (rating) to train a machine learning model to understand positive and negative sentiment. \n",
    "\n",
    "Here you will practice crawling a website collecting the links to each company review page, and finally locate another behind the scenes JavaScript request that gets the review data in a neat json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.1:** Visit the https://www.trustpilot.com/ website and locate the categories page.\n",
    "From this page you find links to company listings.\n",
    "\n",
    "> **Ex. 8.2.2:**\n",
    "Get the category page using the `requests` module and extract each link to a specific category page from the HTML. This can be done using the basic python `.split()` string method. Make sure only links within the ***/categories/*** section are kept, checking each string using the ```if 'pattern' in string``` condition. \n",
    "\n",
    "*(Hint: The links are relative. You need to add the domain name)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.2.1-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.trustpilot.com/categories/'\n",
    "response,call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "links = set()\n",
    "for link_loc in html.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.3:** Get one of the category section links. Write a function to extract the links to the company review page from the HTML.\n",
    "\n",
    "> **Ex. 8.2.4:** Figure out how the pagination is done, by following how the url changes when pressing the **next page**-button to obtain more company listings. Write a function that builds links to paging all the company listing results of each category. This includes parsing the number of subpages of each category and changing the correct parameter in the url.\n",
    "\n",
    "(Hint: Find the maximum number of result pages, right before the next page button and make a loop change the page parameter of the url.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[Answer to Ex.8.2.3-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.trustpilot.com/categories/art'\n",
    "response, _ = connector.get(url,'mapping')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "def get_links(html):\n",
    "    links = set() # define container\n",
    "    for link_loc in html.split('href=\"')[1:]: # locate the start of a link\n",
    "        link = link_loc.split('\"')[0] # split at the end of the link\n",
    "        links.add(link) # if it is: add it to the set container\n",
    "    return links\n",
    "def get_company_links(html):\n",
    "    company_links = [link for link in get_links(html) if '/review/' in link] # check if the /review/ pattern is in the link\n",
    "    return company_links\n",
    "company_links = get_company_links(html)\n",
    "print(len(company_links))\n",
    "def get_all_category_pages(category_link):\n",
    "    response, _ = connector.get(category_link,'mapping_categories')\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "    else:\n",
    "        print('error')\n",
    "        return False\n",
    "    links = get_links(html)\n",
    "    # find the max_page.\n",
    "    page_links = [link for link in links if '?page=' in link] # check if the paging parameter is in the link\n",
    "    if len(page_links)==0: # no pages.\n",
    "        return [category_link]\n",
    "    n_pages = max([int(link.split('page=')[-1]) for link in page_links]) # extract the page value and take the max\n",
    "    paging_links = [category_link] # define container and store the original result page\n",
    "    q = category_link+'?page=%d' # define the varying parameter string.\n",
    "    for num in range(2,n_pages+1): # build the links.\n",
    "        paging_links.append(q%num)\n",
    "    return paging_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.5:** Loop through all categories and build the paging links using the above defined function.\n",
    "\n",
    "> **Ex. 8.2.6:** Randomly pick one of category listing links you have generated, and get the links to the companies listed using the other function defined. \n",
    "\n",
    "> **Ex. 8.2.7:** Visit one of these links and inspect the **>Network Monitor<** to locate the request that loads the review data. Use the requests module to retrieve this link and unpack the json results to a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[Answer to Ex.8.2.5-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex. 8.2.5. Build the paging links\n",
    "company_listings = []\n",
    "for link in links: \n",
    "    if 'support.trustpilot' in link:\n",
    "        continue\n",
    "    company_listings+=get_all_category_pages(link)\n",
    "print('We need to visit %d company listing pages to collect all company addresses'%len(company_listings))\n",
    "# ex. 8.2.6\n",
    "company_links = get_company_links(random.choice(company_listings)) # use the above defined function\n",
    "# ex. 8.2.6\n",
    "direct_link = 'https://www.trustpilot.com/review/59f33de00000ff0005aed171/jsonld'\n",
    "response, call_id = connector.get(direct_link, 'download_review')\n",
    "d = response.json() # parse json using the build-in function.\n",
    "df = pd.DataFrame(d[0]['review'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on coming this far. By now you are almost - still need to figure out how to page the reviews and to find the company ID in the html -, ready to deploy a scraper collecting all reviews on trustpilot. \n",
    "If you wanna see just how valuable such data could be visit the follow blogpost: https://blog.openai.com/unsupervised-sentiment-neuron/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
